{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tworzenie środowiska Composer"
      ],
      "metadata": {
        "id": "aTOvEttklpzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my-composer-environment # Nazwa\n",
        "data_stream_airflow # Zasobnik niestandardowy\n",
        "europe-central2 # Lokalizacja\n",
        "europe-central2-c # Strefa"
      ],
      "metadata": {
        "id": "Ebeva40VmSGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ustawianie zmiennych środowiskowych Apache Airflow\n",
        "\n",
        "- Zmienne Apache Airflow `gcp_project, gcs_bucket, gce_zone` to specyficzna koncepcja Airflow, która różni się od zmiennych środowiskowych.\n",
        "\n",
        "- Ta metoda nie działa!"
      ],
      "metadata": {
        "id": "SbpVVqzLnYuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COMPOSER_INSTANCE=my-composer-environment"
      ],
      "metadata": {
        "id": "XLnn5N-ToTTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID=third-essence-345723"
      ],
      "metadata": {
        "id": "YG-jkdhToutb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GCS_BUCKET=gs://data_stream_airflow"
      ],
      "metadata": {
        "id": "Dkup6WjEpapV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GCE_ZONE=europe-central2-c"
      ],
      "metadata": {
        "id": "Qo8UHWewqS9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcloud composer environments run ${COMPOSER_INSTANCE} \\\n",
        "    --location europe-central2 variables -- --set gcp_project ${PROJECT_ID}"
      ],
      "metadata": {
        "id": "SmhU7Hc2ooRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcloud composer environments run ${COMPOSER_INSTANCE} \\\n",
        "    --location europe-central2 variables -- --set gcs_bucket ${GCS_BUCKET}"
      ],
      "metadata": {
        "id": "bvPk00ZipLpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcloud composer environments run ${COMPOSER_INSTANCE} \\\n",
        "    --location europe-central2 variables -- --set gce_zone ${GCE_ZONE}"
      ],
      "metadata": {
        "id": "gDbXfA17qOv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Użycie interfejsu użytkownika Airflow:\n",
        "- Zaloguj się do interfejsu użytkownika Airflow w Cloud Composer.\n",
        "- Przejdź do sekcji Admin > Variables i dodaj/edytuj zmienne ręcznie."
      ],
      "metadata": {
        "id": "HBX4BMkIv2bI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DAG"
      ],
      "metadata": {
        "id": "lOIkvJLMlG1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtHiQsWzG6LO"
      },
      "outputs": [],
      "source": [
        "# Importowanie bibliotek i modułów\n",
        "import datetime  # Importowanie modułu datetime do manipulacji datami i czasami.\n",
        "import os  # Importowanie modułu os do obsługi zmiennych środowiskowych i operacji na ścieżkach.\n",
        "\n",
        "from airflow import models  # Importowanie models z Airflow do tworzenia DAG-ów.\n",
        "from airflow.contrib.operators import dataproc_operator  # Importowanie operatorów Dataproc do zarządzania klastrami Dataproc.\n",
        "from airflow.utils import trigger_rule  # Importowanie trigger_rule do definiowania warunków uruchamiania zadań.\n",
        "\n",
        "# Określenie ścieżki wyjściowej dla wyników zadania Dataproc w Google Cloud Storage.\n",
        "output_file = os.path.join(\n",
        "    models.Variable.get('gcs_bucket'), 'wordcount',\n",
        "    datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep  # Ścieżka pliku jest dynamiczna i zawiera znacznik czasu.\n",
        "\n",
        "# Ścieżka do przykładowego pliku JAR Hadoop wordcount dostępnego na każdym klastrze Dataproc.\n",
        "WORDCOUNT_JAR = (\n",
        "    'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'\n",
        ")\n",
        "\n",
        "# Argumenty przekazywane do zadania Dataproc.\n",
        "input_file = 'gs://pub/shakespeare/rose.txt'  # Ścieżka do pliku wejściowego w GCS (zawierającego tekst).\n",
        "wordcount_args = ['wordcount', input_file, output_file]  # Argumenty dla zadania Hadoop.\n",
        "\n",
        "# Ustalenie daty rozpoczęcia DAG-a (dzień wczorajszy).\n",
        "yesterday = datetime.datetime.combine(\n",
        "    datetime.datetime.today() - datetime.timedelta(1),\n",
        "    datetime.datetime.min.time())\n",
        "\n",
        "# Argumenty domyślne dla DAG-a.\n",
        "default_dag_args = {\n",
        "    'start_date': yesterday,  # Data startowa.\n",
        "    'email_on_failure': False,  # Wyłączone wysyłanie e-maili przy błędach.\n",
        "    'email_on_retry': False,  # Wyłączone wysyłanie e-maili przy ponownych próbach.\n",
        "    'retries': 1,  # Liczba ponownych prób w przypadku błędu zadania.\n",
        "    'retry_delay': datetime.timedelta(minutes=5),  # Opóźnienie między próbami.\n",
        "    'project_id': models.Variable.get('gcp_project')  # Identyfikator projektu GCP pobrany z zmiennych Airflow.\n",
        "}\n",
        "\n",
        "# Definicja DAG-a.\n",
        "with models.DAG(\n",
        "        'composer_hadoop_tutorial',  # Nazwa DAG-a.\n",
        "        schedule_interval=datetime.timedelta(days=1),  # Harmonogram uruchamiania - raz dziennie.\n",
        "        default_args=default_dag_args) as dag:\n",
        "\n",
        "    # Zadanie tworzenia klastra Cloud Dataproc.\n",
        "    create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(\n",
        "        task_id='create_dataproc_cluster',  # Unikalny identyfikator zadania.\n",
        "        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',  # Nazwa klastra z dynamicznie generowaną datą.\n",
        "        num_workers=2,  # Liczba węzłów roboczych.\n",
        "        zone=models.Variable.get('gce_zone'),  # Strefa GCE pobrana z zmiennych Airflow.\n",
        "        master_machine_type='n1-standard-1',  # Typ maszyny dla węzła master.\n",
        "        worker_machine_type='n1-standard-1')  # Typ maszyn dla węzłów roboczych.\n",
        "\n",
        "    # Zadanie uruchomienia przykładu Hadoop wordcount.\n",
        "    run_dataproc_hadoop = dataproc_operator.DataProcHadoopOperator(\n",
        "        task_id='run_dataproc_hadoop',  # Unikalny identyfikator zadania.\n",
        "        main_jar=WORDCOUNT_JAR,  # Ścieżka do pliku JAR Hadoop.\n",
        "        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',  # Nazwa klastra.\n",
        "        arguments=wordcount_args)  # Argumenty dla zadania.\n",
        "\n",
        "    # Zadanie usunięcia klastra Cloud Dataproc.\n",
        "    delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(\n",
        "        task_id='delete_dataproc_cluster',  # Unikalny identyfikator zadania.\n",
        "        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',  # Nazwa klastra.\n",
        "        trigger_rule=trigger_rule.TriggerRule.ALL_DONE)  # Reguła uruchamiania - uruchom nawet w przypadku błędu poprzednich zadań.\n",
        "\n",
        "    # Definicja zależności między zadaniami.\n",
        "    create_dataproc_cluster >> run_dataproc_hadoop >> delete_dataproc_cluster"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import os\n",
        "from airflow import models\n",
        "from airflow.providers.google.cloud.operators.dataproc import DataprocCreateClusterOperator, DataprocSubmitJobOperator, DataprocDeleteClusterOperator\n",
        "from airflow.utils import trigger_rule\n",
        "\n",
        "# Ścieżka wyjściowa dla zadania Dataproc\n",
        "output_file = os.path.join(\n",
        "    models.Variable.get('gcs_bucket'), 'wordcount',\n",
        "    datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep\n",
        "\n",
        "# Ścieżka do przykładowego pliku JAR Hadoop wordcount\n",
        "WORDCOUNT_JAR = 'gs://hadoop-jar-files/hadoop-mapreduce-examples.jar'\n",
        "\n",
        "# Plik wejściowy dla zadania\n",
        "input_file = 'gs://pub/shakespeare/rose.txt'\n",
        "\n",
        "wordcount_args = ['wordcount', input_file, output_file]\n",
        "\n",
        "# Data startowa dla DAG\n",
        "yesterday = datetime.datetime.combine(\n",
        "    datetime.datetime.today() - datetime.timedelta(1),\n",
        "    datetime.datetime.min.time())\n",
        "\n",
        "# Domyślne argumenty dla DAG\n",
        "default_dag_args = {\n",
        "    'start_date': yesterday,\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': datetime.timedelta(minutes=5),\n",
        "    'project_id': models.Variable.get('gcp_project')\n",
        "}\n",
        "\n",
        "# Definicja regionu (należy ustawić odpowiednią zmienną Airflow lub podać region bezpośrednio)\n",
        "region = models.Variable.get('dataproc_region', 'europe-central2')  # Ustaw domyślny region\n",
        "\n",
        "with models.DAG(\n",
        "        'composer_hadoop',\n",
        "        schedule_interval=datetime.timedelta(days=1),\n",
        "        default_args=default_dag_args) as dag:\n",
        "\n",
        "    create_dataproc_cluster = DataprocCreateClusterOperator(\n",
        "        task_id='create_dataproc_cluster',\n",
        "        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n",
        "        num_workers=2,\n",
        "        zone=models.Variable.get('gce_zone'),  # Ustaw strefę z odpowiedniej zmiennej\n",
        "        master_machine_type='n1-standard-2',\n",
        "        worker_machine_type='n1-standard-2',\n",
        "        region=region,  # Dodano region\n",
        "        internal_ip_only=False  # Ustawienie internal_ip_only na false\n",
        "    )\n",
        "\n",
        "    run_dataproc_hadoop = DataprocSubmitJobOperator(\n",
        "        task_id='run_dataproc_hadoop',\n",
        "        job={\n",
        "            \"placement\": {\"cluster_name\": 'composer-hadoop-tutorial-cluster-{{ ds_nodash }}'},\n",
        "            \"hadoop_job\": {\n",
        "                \"main_class\": \"org.apache.hadoop.examples.WordCount\",\n",
        "                \"jar_file_uris\": [WORDCOUNT_JAR],\n",
        "                \"args\": wordcount_args,\n",
        "            },\n",
        "        },\n",
        "        region=region  # Dodano region\n",
        "    )\n",
        "\n",
        "    delete_dataproc_cluster = DataprocDeleteClusterOperator(\n",
        "        task_id='delete_dataproc_cluster',\n",
        "        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n",
        "        trigger_rule=trigger_rule.TriggerRule.ALL_DONE,\n",
        "        region=region  # Dodano region\n",
        "    )\n",
        "\n",
        "    create_dataproc_cluster >> run_dataproc_hadoop >> delete_dataproc_cluster"
      ],
      "metadata": {
        "id": "bpOu5wylx-_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}